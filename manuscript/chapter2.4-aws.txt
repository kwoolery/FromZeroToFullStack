## 2.4 Amazon Web Services (AWS)

AWS is an on-demand cloud computing platform from Amazon, offering dozens of services for serving and managing applications in a robust environment. With AWS, developers and systems engineers don't need to worry about the low-level details of hardware provision and deployment, since AWS provides an easy interface for bringing up new servers as-needed based on the immediate requirement of your application. In addition to the basic server management services, AWS provdides services for computing, storage, networking, database, analytics, application services, deployment, management, mobile, developer tools, and tools for the Internet of Things, as well as services to make machine learning accessible at scale. 

For this project, we'll be using AWS for media file storage, serving our API, as well as hosting our datastore. Most AWS services have a free-tier that allows developers to take the services for a test run without incurring any costs. I'll attempt to point out the thresholds that fall within this free-tier, and steer the project in a direction that won't require any initial investment when possible. As of the writing of this book, the following services are provided on the free tier with the given limitations. 

* Amazon Simple Storage Service (S3) - 5 GB Storage
* Amazon Elastic Compute Cloud (EC2) - 750 Hours of compute time per month on a t2.micro instance
* Amazon Elastic Container Registry (ECR) - 500MB of storage for 1 year
* Amazon Elastic Container Service (ECS) - 750 Hours of EC2 compute time per month on a t2.micro instance
* Amazon RDS - 750 Hours per month of a db.t2.micro database server
* Amazon DynamoDB - 25 GB Storage
* AWS Lambda - 1 Million free requests per month
* AWS Code Build - 100 Minutes
* AWS CodePipeline - 1 Active Pipeline Per Month

### 2.4.1 Signing up for AWS

Navigate to the following url to signup for your free-tier AWS account: 
<https://portal.aws.amazon.com/billing/signup?redirect_url=https%3A%2F%2Faws.amazon.com%2Fregistration-confirmation#/start>

{height=50%, float=center}
![AWS Signup Page](images/chapter2.4-aws/aws-signup.png)

Enter your information, and follow the directions to the next page to enter your contact information. When choosing an account type, you should choose personal to begin with, then proceed to the billing page.

The billing page is going to ask you for your credit card information. Don't be alarmed, this information is only required if your usage exceeds the free tier limits as discussed above. You can look into advanced features of AWS that allow you to send alerts if usage is approaching the limits so you can make a decision to disable the service if necessary.

Once you input your billing information, you'll be asked to provide a phone number for verification. You'll get a call from AWS to verify a pin number presented to you on the sign up page. Just input the number, and you'll be successully verified and taken the the support plan page.

Choose the "Basic Plan" since we won't initially need support from Amazon, however, if you do need support in the future, you can choose to change plans to the one that works for your situation.

{width=85%, float=center}
![AWS Support Plan](images/chapter2.4-aws/aws-support-plan.png)

If everything goes well, you'll be presented with the following welcome page. From here, you can sign into the AWS Console where you'll manage all the services by clicking the "Sign In to the Console" button.

{width=85%, float=center}
![AWS Welcome Page](images/chapter2.4-aws/aws-welcome.png)

You'll be taken to the login page. 

{height=50%, float=center}
![AWS Sign In](images/chapter2.4-aws/aws-signin.png)

Input the email you used during signup, then click 'Next' to be taken to the password page.

{height=50%, float=center}
![AWS Password](images/chapter2.4-aws/aws-password.png)

Finally, if the login succeeded, you'll be shown the AWS Console Dashboard page. You can choose to initialize, modify, and monitor all of the services provided by AWS. 

{width=85%, float=center}
![AWS Dashboard](images/chapter2.4-aws/aws-dashboard.png)

Congratulations, you've successfully signed up for Amazon Web Services!!! Everything you need to be successful is now at your fingertips. 

To being, We're going to be setting up the Simple Storage Service, or more commonly known as S3, for use in our application to store uploaded media from customers.  

### 2.4.2 AWS Identity and Access Management (IAM)

In order access the services of your account, you need to configure the local tools to use the proper credentials. You need to have an AWS Access Key ID, and an AWS Secret Access Key. 

To setup the proper credientials, navigate to the IAM console at <https://console.aws.amazon.com/iam/home?#/home>, and click on the Users option in the left hand menu. If there are no users listed in the table, click on the Add user button in order to add a new user. Create a new user called CoffeeShopRankerAPIUser, and make sure the Programattic Access option is checked in the Access Type section. Click on the Next: Permissions button. 

On the permissions page, we need to create a group to add the newly created user to in order to grant AWS service access for the user. Click on the Create group button to get an overaly of all services available. In the Group name field, call the group CoffeeShopRankerAPIGroup. We're going to assign the following permissions to the newly created Group: 

AmazonS3FullAccess
AmazonECS_FullAccess
AmazonDynamoDBFullAccess
AmazonEC2FullAccess
AmazonEC2ContainerRegistryFullAccess

Once these permissions have been selected, click on the Create group button. Make sure the newly formed group is selected, and click on the Next:Review button. If everything looks good on the review page, click on the Create user button. 

If the user was created successfully, you'l be taken to the user listing page where you should see the newly created user along with the Access key ID, and the Secret access key. Copy these values to a safe place, you'll use them later when intalling the AWS Command Line Tools. 

### 2.4.3 AWS Command Line Tools (awscli)

In order to automate the build, and deployment process, you'll need to install the AWS Command Line Tools. The AWS Command Line Tools are an open source tool that provides convenient commands for interacting with AWS services. You can use the tool to manage files on S3, deploy new containers to ECS, and to interact with DynamoDB among many other tasks. For more information on the tool, please visit <https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-welcome.html> for complete documentation. For complete installation instructions, navigate to <https://docs.aws.amazon.com/cli/latest/userguide/installing.html>, and for complete configuration options, you can check out <https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html>. The following is a rough overview of how to get the AWS Command Line Tools installed. 

In order to install AWS Command Line Tools, you'll need Python 2, version 2.6.5+, or Python 3 version 3.3+, as well as the pip package manager. You can manually install the tool, however for simplicity, I'll assume that you have pip installed. If you need instructions on installing pip, please see the documentation at <https://pip.pypa.io/en/stable/installing/#installing-with-get-pip-py>. In the simplest form, installing pip should be as easy as running the following from a terminal on any operating system, and following the instructions. If you're using python3, replace the python call with python3.

'''
curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py
python get-pip.py
'''

Once, you've verified that pip is installed, you can run the following command to install the tool. If you're using python3, replace the python call with python3.

'''
pip install awscli --upgrade --user
'''

Once the awscli package has been installed, you can verify that it's been installed correctly by running the following command. 

'''
aws --version
'''

You should get an output showing the current version of the AWS CLI, and which version of Python it's associated with. If you get something like "AWS CLI 1.11.84 (Python 3.6.1)", congratulations, you've installed the AWS Command Line Tools. 

In order to use the AWS Command Line Tools, you need to configure the proper credentials to be used. Run the following command to create a new credential profile to be used.

'''
aws configure --profile coffeeshopranker
'''

You'll be asked to input your AWS Access Key ID, and your AWS Secret Access Key. You can use the keys created for the CoffeeShopRankerAPIUser in the IAM section 2.4.2 above. 

Once you've created the profile, you can use this specific profile with the --profile argument on aws and be granted access to the appropriate services. 

Congratulations, you've successfully setup command line access to AWS. Now on to the exciting services. 

### 2.4.4 Simple Storage Service (S3)

Amazon S3 is a massively scalable object storage system built to store and retrieve any amount of data from anywhere. It can be used as a simple key-value store, or as a full-blown filesystem where files can be accessed directly given a fully qualified path. It can even be setup to serve a static wEBite with automatic index, and error pages, and redirects if necessary. We're not going to go into all of the features of S3 in this section, since there are an infinite number of options for storage, access, security, metadata, etc. If you're interested in knowing more about S3, I would recommend reading the Amazon S3 FAQ at https://aws.amazon.com/s3/faqs/?nc=sn&loc=6. 

In order to setup S3 for our use, we need to navigate to the S3 management console. From anywhere in the AWS Console, you can pull down the "Services" menu at the top left, and either choose S3 from the "Storage" second, or you can start typing S3 in the "Find a service by name or feature" input box. 

When you navigate to the S3 console for the first time, you'll see that you don't have any buckets. Buckets are globally unique containers for everything you store in Amazon S3. A globally unique container means that you must create a name for your bucket that nobody else has ever used. If you choose common words, AWS will probably complain that someone has used that name before. 

Here are two acceptible strategies for determining bucket names. The first, and simplest is to use your username as a prefix to the bucket name. In my case, my username is kevintruvu, so all of my buckets will have the name kevintruvu.<name of my bucket>. The likelyhood that someone has used this bucket name is much lower, since my username is unique to AWS as well. The second strategy should be used when you want to make the contents of the bucket available publicly by way of a unique hostname you have access to. This is helpful when hosting static wEBites from S3, but can be used for media delivery as well. An example would be if I wanted to have all the files in the bucket accessible from http://media.example.com/. In this case, I would name my bucket media.example.com, and setup a CNAME record for media.example.com to point to the appropriate S3 url (!!!FILL IN S3 URL!!!). 

Once, you've selected a naming strategy, click on the "+ Create bucket" button. You'll be taken to the following screen, where you can enter your DNS Compliant bucket name. I've decided to name my bucket kevintruvu.coffeeshopranker in order to guarantee uniqueness.

!!! IMAGE !!!

Click Next to configure the bucket options. You can leave the default options in place, and click Next to get to the permissions section. 

Since we're going to be using S3 to store, and deliver customer generated images, we'll need to allow end users to access the images directly. We'll give the public read access, and we'll use our application with our API key to upload images by way of the application. Go to the "Manage public permissions" settings, and choose the "Grant public access to this bucket" option. You'll be presented with a warning that this bucket will have public access, which is what we're trying to achieve. Click Next to move to the review section.

If everything looks as you expected, click on the "Create Bucket" button. 

If everyting was accepted, you'll be taken back to the S3 Console Dashboard where you'll se a list of available buckets. You should notice your newly created buck in the list, and the Access field should be marked as public. You can click on the bucket to enter the S3 Bucket Management Console. 

Since you haven't uploaded anything into the bucket yet, you'll be presented with an empty bucket. From here, you can create a new folder, or upload files, and folders directly. 

We don't have any files to upload yet, so let's create a folder. Click on the "+ Create folder" button. You'll be asked to name the folder, and whether you want to encrypt the contents. For our case, just name the folder "Test Folder" and click the Save button. 

You should now see your folder in the list of objects in the overview. Click on the folder to see the contents of the folder. From the listing page, you can either create another folder, or upload files. Click on the "Upload" button, and choose a file from your file system to upload by clicking the Add Files button. You can choose one or more files from your file system. You'll see the files that are ready to upload. Click Next to set the permissions of the file. 

On upload permissions section, you may notice that there's a "Manage public permissions" section similar to when you created the bucket. When uploading files from this interface, you have to choose permissions explicitly. There are ways to automate this when uploading from the command line, or from code, but for now in the manual upload form, you should choose "Grant public read access to this object" From the dropdown, and click Next to set the properties.

You can choose different storage options from the properties page, and whether the file will be encrypted, however, we're going to leave all the default properties for this file. If you want more information regaring the storage classes, refer to the S3 FAQ at  https://aws.amazon.com/s3/faqs/?nc=sn&loc=6. Click Next to get to the review page.

If everything looks good, click on the Upload button. Once the upload is successful, you should see the file listed in the directory. Clicking on the filename will take you to the overview page for that file. 

There are various actions that can be taken on the file such as downloading the file directly, or making the file public if it's not already. If you click open, you'll be able to see the contents of the file in your browser, even if the file is private. However, if the file is public, you'll be presented with a Link at the bottom of the page. You should see something similar to https://s3.amazonaws.com/kevintruvu.coffeeshopranker/testfolder/graph.png. It should become obvious now why the bucket name is so critical, all objects in S3 standard storage are accessible via the s3.amazonaws.com hostname, where the top level directory is your bucket name. 

If you were able to see the file in a browser, congratulations, you've successfully setup S3, and you now have the ability to upload and view media from a public url with only a few steps. 

We'll mostly be dealing with S3 in an automated fashion throughout the rest of the project, since we'll be uploading and viewing files from applications rather than directly from web pages, we'll be using the Go AWS SDK to accomplish this. We'll also touch on the AWS command line tools in order to list, and manage files in S3 in a similar way as would be done in the local file sytem. 

### 2.4.5 Elastic Compute Cloud (EC2)

Amazon EC2 is a scalable computing platform that provides virtual server instances for quickly scaling based on your application requirements. When launching an EC2 server, you can decide the type of machine, and the software that is preloaded by using an Amazon Machine Image (AMI). You can start with a fully loaded instance, or you can start with a barebones instance and add all the requirements yourself, and save your image as an AMI for quickly launching new instances with your requirements built in. If you need detailed information about all the features EC2 offers, you can read the EC2 documentation at <https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts.html>. We'll mostly focus on setting up a single free-tiered instance in order to show how to setup a general purpose server for local staging, and testing. Our app will eventually use the ECS (Elastic Container Service) to host our application Docker image. When we setup ECS with EC2, you should make sure that the EC2 instace we setup in this section is stopped in order to not incure additional charges.  

The first step is to navigate to the EC2 Dashboard by searching for the service name in the Services dropdown from anywhere in the AWS console. You should find yourself on the EC2 Dashboard. 

{width=85%, float=center}
![EC2 Dashboard](images/chapter2.4-aws/ec2/ec2-landing.png)

Once on the EC2 Dashboard, click the Launch Instance button. You'll start the process of building your EC2 instance by first choosing which AMI to use. You can choose from a variety of available images, either for free, or you can navigate in the AWS Marketplace for exactly what you may be looking for. Keep in mind that the Marketplace images will typically charge based on compute time usage. For this project, we're going to go with one of the Quick Start images, and install what we need from scratch. 

{width=85%, float=center}
![EC2 Choose AMI](images/chapter2.4-aws/ec2/ec2-step1-ami.png)

We want to make sure the image we choose is marked as Free Tier Eligible. For this project, you should use the Ubuntu Server 18.04 LTS (HVM), SSD Volume Type - ami-0ac019f4fcb7cb7e6 image. Click on the Select button to continue to Choose an Instance Type step.

{width=85%, float=center}
![EC2 Ubuntu Selection](images/chapter2.4-aws/ec2/ec2-ubuntu-choose.png)

By default, the free tier t2.micro instance will be selected. If you want more information about the EC2 instance types, you can find details about the dozens of types at <https://aws.amazon.com/ec2/instance-types/>. The t2.micro is a single CPU virtual machine with 1GB of RAM. It's not a powerhouse, and wouldn't typically be suitable for a production application, however it's ideal for free prototyping, and it's free, which is a huge bonus. For this project, we're going to use the default configuraion settings, we do have to update the default security group though. You can get to the security group page by clicking on the Configure Security Group tab on the top progress bar. 

{width=85%, float=center}
![EC2 Security Group Tab](images/chapter2.4-aws/ec2/ec2-configure-security-tab.png)

Security groups on AWS are extremely flexible and allow you control port level access to your virtual servers. By default the SSH port 22 is open to all IP addresses. We'll leave that in place for now. 

{width=85%, float=center}
![EC2 Security Warning](images/chapter2.4-aws/ec2/ec2-security-warning.png)

You'll notice a warning about limiting access to specific IP addresses from the review page. We're going to be accessing this server from mobile devices, and web browsers, so we need to make sure to allow all IP addresses. You'll see a warning about closing off access to all IP addresses. If you know your local IP address, it may make sense to limit SSH access, but for now, we'll leave this setting alone. Click on the Add Rule button.

{width=85%, float=center}
![EC2 Security Add Rule](images/chapter2.4-aws/ec2/ec2-security-add-port.png)

In order for our application to communicate with our server, we're going to need a port open to the public. Leave the type set to Custom TCP, change the Port Range value to 11373, and update the CIDR input to use 0.0.0.0/0. Use "Coffee Ranker App Comm Channel" for the description. Once you've setup the security rule, click on the blue Review and Launch button. 

On the Review Page, you'll once again be presented with the security warning about your server being open to the world. Once you've reviewed, launched, and accessed your server, you can choose to come back and update the security restrictions, but for this application, we're going to accept the risks, however, in a real world environment, you would absolutely want to limit access in order to avoid any security risks. 

{width=85%, float=center}
![EC2 Review Page](images/chapter2.4-aws/ec2/ec2-review-2.png)

The most important thing to review is to make sure the instance type you've selected is t2.micro. Anything else in the instance type will cost you based on compute type, so confirm that you have the free tier instance selected. If all the other information looks accurate in the Review page, go ahead and click the Launch button. You should get a dialog asking you to select or create a new key pair. 

{width=85%, float=center}
![EC2 Select Key Pair](images/chapter2.4-aws/ec2/ec2-select-keypair.png)

In order to provide an additional level of security, AWS requires a key pair to access the EC2 instances, at least initially. You can create a new key pair from this dialog. From the top dropdown, choose the "Create a key pair" option. This option will require you to name your key pair. For this key pair, set "Coffee Ranker Key Pair" as the name, then click on the Download Key Pair button. Store the downloaded pem file in a secure location, because you won't be able to download this file again. If you lose the file, you'll have to go about creating a new key pair, and updating the instance.

Once you have the key pair in a safe location, click on the Launch Instances button in the overlay. If everything went as expected, you should be taken to a Launch Status page. 

{width=85%, float=center}
![EC2 Launch Status](images/chapter2.4-aws/ec2/ec2-launch-status.png)

You can check on the instance by clicking on the instance link in the box with the title "Your instances are now launching." Once you click on the button, you'll be taken to the instances listing page. This is where you will access all of your running instances. You should see your new server, and the current instance state. 

In order to connect to your newly launced server, you need to ssh into the machine by using the key pair file you downloaded earlier. If you select the instance, and click the Connect button, you'll be presented with detailed instructions on how to connect. 

{width=85%, float=center}
![EC2 Connection Instructions](images/chapter2.4-aws/ec2/ec2-connect-instructions.png)

The following instructions for connecting are specific to Mac, and Linux, with a slight variation for Windows with Cygwin. If you're using something like PuTTY for Windows, you'll have to find the instuctions for connecting over SSH using a private key pair. 

The first step is to make sure the pem file you downloaded is in the correct location, and has the appropriate restricted permissions. You can do this by opening a terminal window, finding the pem file, and copying it to the .ssh directory in your home directory. Once the file is in the .ssh directory, run the following to change permissions. 

```
chmod 400 CoffeeRankerKeyPair.pem
```

This will give only your user access to read the file. You can now move up to your home directory, and access your server. Remember, your server hostname will be different than what is used in this example, just replace it with your hostname, and you should be able to access your server. You should also note the actual filename of the pem file. Run the following, or a variation based on your settings.

```
ssh -i ~/.ssh/CoffeeRankerKeyPair.pem ubuntu@ec2-54-165-222-48.compute-1.amazonaws.com
```

Since this is the first time you'll be accessing the server, you'll be presented with the following question. Type 'yes' to proceed. 

```
The authenticity of host 'ec2-54-165-222-48.compute-1.amazonaws.com (54.165.222.48)' can't be established.
ECDSA key fingerprint is SHA256:rP1PWuDqGRzSQeAoGHBmocJl1P+Ol526oJ+NGZ9wA6I.
Are you sure you want to continue connecting (yes/no)?
```

If you've setup everything correctly, you should get access to the command line prompt as the 'ubuntu' default user.  Congratulations, you've successfully setup your first EC2 server instance!

**In order to avoid additional charges, and to be taken out of the free tier, you should take down the EC2 Instance. We'll be setting up an Elastic Container Service Cluster with a single t2.micro instance in section 2.4.7. **

### 2.4.6 Elastic Container Registry (ECR)

In order to streamline deployment of our application, we need to host our Docker Image on a host that's accessible from our build system (AWS CodePipeline). You have a choice as to where to host, and some choose DockerHub, which is the official repository for Docker Images, however, Amazon AWS provides ECR for hosting your containers. 

We're going to work with our test image we created in section 2.3.2. If you haven't created the image, go back to that section, and follow along to create the coffeeshop-ranker-docker-demo image. 

Once you've successfully created the image, the next step is to push it to ECR. 

!!! INSERT IMAGE !!!
First, navigate in the AWS Management Console to the ECR Managment Page, and click on the Get started button. 

On the Configure repository form, create a new repository name, in this case, you can use the same name as the image coffeeshop-ranker-docker-demo. Click on the Next step button to get a thorough set of instructions for building, tagging, and pushing your Docker image. You'll be required to use the AWS Command Line Tools, so make sure you followed along with sections 2.4.2, and 2.4.3 above to get the tool setup correctly.

For this walkthrough, I'm going to follow the macOS and Linux commands necessary to get our image into ECR. If you're a Windows PowerShell user, you can find those instructions in parallel on the "View Push Commands" section. 

First, retrieve the login command to authenticate your Docker client to your registry by running: 

'''
$(aws --profile coffeeshopranker ecr get-login --no-include-email --region us-east-1)
'''

This is a shell command that runs the docker login command with the appropriate settings for AWS. You can remove the $() in order to get the actual command to run independently. Once this command is run, you should see that Login Succeeded before moving onto the next step. 

The next step is to build the current Docker Image from the directory where the Docker file exists, and tag it appropriately for ECR by running the following command. You can skip this step if you built the image as per the instructions in section 2.3.

'''
docker build -t coffeeshop-ranker-docker-demo .
'''

You should have a new image tagged as coffeeshop-ranker-docker-demo:latest. The next step is to tag the image to push the image to ECR by running the following command, or a variation depending on the ECR server you're being directed to. 

'''
docker tag coffeeshop-ranker-docker-demo:latest <ecr.server.url>/coffeeshop-ranker-docker-demo:latest
'''

Once you've tagged your image for ECR, it's time to push the image to the newly created repository by running the following command, again, you're ECR server will be unique to your repository.

'''
docker push <ecr.server.url>/coffeeshop-ranker-docker-demo:latest
'''

You should see each one of the dependent images being pushed to ECR, followed by the sha256 hash showing a successful deployment of the repository. If you navigate to the ECR listing, and choose coffeeshop-ranker-docker-demo from the list of repositories, you should see the latest image that you just deployed. Keep the Repository URI handy for setting up the ECS Container in the next section. Congratulations, you've successfully deployed a Docker Container to ECR. Onto getting it running in a Container Cluster. 


### 2.4.7 Elastic Container Service (ECS)

Amazon ECS is a highly scalable, high performance container orchestration service that supports running Docker containers in distributed clusters. For more detailed information about Amazon ECS, check out <https://aws.amazon.com/ecs/> 

We'll setup a very basic free tiered version of ECS with a single EC2 t2.micro instance. Navigate to the ECS Management console at <https://console.aws.amazon.com/ecs/home?region=us-east-1#/clusters>

Click on 'Create Cluster' Button to define your cluster. One the Cluster Template page, choose the EC2 Linux + Networking option. 

On the Configure Cluster page, input CoffeeShopRankerECSCluster as your cluster name, and make sure you choose the On-Demand Instance as your Provisioning Model. Most importantly, make sure the choose the t2.micro EC2 instance type.  

For the Key Pair options, choose Coffee Ranker Key Pair, or go to section 2.4.5 to follow the directions on creating a new Key Pair. 

For the Networking Section, either choose to create a new VPC, or choose one that was created by following section 2.4.5. Once you choose a VPC, select the Subnet, and the Security Group. Keep a reference to the VPC ide, and the Subnet ID for setting up a public interface through the Elastic Load Balancer. You can choose the Security Group as created in section 2.4.5, or create a new one. If you choose one, follow the link to the Rules for that Security Group, and make sure that the inbound rules are setup to allow HTTP(80) from all sources (0.0.0.0/0). You'll also want to add another rule to allow SSH(22) from all sources. Click save, and go back to the ECS setup page. 

Click on the Create button in order to finishing creating the cluster. If everything is setup correctly, you should see a Launch status page with three green boxes for setting up the ECS cluster, the ECS Instance IAM Policy, and the CloudFormation stack. 

Click on the View Cluster button to be taken to the Cluster Details page. From here we'll need to create a new Task Definition to allow our Cluster to run our deployed image. 

Click on the Task Definitions menu item, then click on Create new Task Definition button. Make sure to choose the EC2 launch type in order to utilize the EC2 free tier, and our t2.micro we brought up with the cluster. Click on Next step to proceed to the configuration page. 

Enter CoffeeShopRankerTask as the Task Definition Name, and choose the ecsTaskExecutionRole as the Task Role. Leave the Network Mode set as <default>. Choose ecsTaskExecutionRole as the Task execution role. Leave Task Memory, and Task CPU blank. 

Under Container Definitions, click on the Add container button. You'll be presetned with an Add Container Form. Set the Container name to CoffeeShopRankerContainer. Input the Image URI from the end of section 2.4.7 above. For Memory Limits, choose a Hard Limit of 300 MiB. For Port Mappings, input host port 80 and container port 80. Scroll to the bottom of the form, and click the Add button. Then click on the Create button on the Task Definition Page. You should be directed to a page that shows that your Task Definition has been created successfully. 

Navigate back to the Cluster Details page, and click on the Create button on the Services tab. On the Configure service page, choose EC2 as the launch type, and make sure the Task Definition points the newly created Task Definition. Set the Service name to CoffeeShopRankerService. Choose DAEMON as the Service Type, then click on the Next step button.

--- Create a load balancer
--- Follow all the steps!!!


### 2.4.8 DynamoDB
Amazon DynamoDB is a non-relational, NoSQL database built for performance and scale. NoSQL means that data is typically not stored by default in a tabular way, and it's accessible directly by keys. By directly accessing the data with keys rather than complex queries, DynamoDB acts as a very fast, distributed hashtable with essentially O(1) performance for data access. In contrast, a SQL based relational database has internal data structures, and the Structured Query Language (SQL) that allow for general random access of any view into the data at any time in the future, while paying a price in performance. One of the most important parts of setting up a NoSQL database is the key design schema, since it will be your responsibility, as the application developer, to pre-define how data is accessed. Any modifications to data access will require new key design in order to maintain the expected performance, and efficiency provided by NoSQL databases.

For more information about the details of DynamoDB, please visit <https://docs.aws.amazon.com/dynamodb/index.html> for throrough documentation.

The first step in setting up DynamoDB is navigate to the DynamoDB Dashboard, and setup a new table for our datastore.

{width=85%, float=center}
![DynamoDB Landing](images/chapter2.4-aws/dynamodb/dynamodb-landing.png)

Once you're on the Dashboard, click on the Create Table button. We'll go into much more detail in the section on Datastore Design, however, to get a feel for DynamoDB, we'll create a very simple table to begin with that holds the User Inforamtion. 

{width=85%, float=center}
![DynamoDB Create Table](images/chapter2.4-aws/dynamodb/dynamodb-create-table.png)

Enter "Users" into the Table name input field. Enter "UserID" into the Primary key field, and change the type dropdown to Number. This sets the primary key to UserID, which will be used to directly access the User information. Make sure the Add sort key checkbox is checked, and input "Location" as the sort key, and make sure the type dropdown is set to String. Make sure that the Use default settings checkbox is checked. Click on the Create button. This will create a very basic table we can use for testing access when setting up our middle-tier. 

Once the table is created successfully, you should see the following Tables management screen. You can check the status of the table from here, as well as managing backups, inserting items, searching for items, managing alerts, and setting up indexes. We'll go into more detail on these functions in the section on datastore design.  

{width=85%, float=center}
![DynamoDB Table Created](images/chapter2.4-aws/dynamodb/dynamodb-table-created.png)

Make sure your newly created Users table is selected, and click on the Items tab on the right panel. 

{width=85%, float=center}
![DynamoDB Items Tab](images/chapter2.4-aws/dynamodb/dynamodb-item-listing-before.png)

Click on the Create Item button. You will be presented with the following overlay, where you can add a new item directly into the database. In order to 

{width=85%, float=center}
![DynamoDB Create Item](images/chapter2.4-aws/dynamodb/dynamodb-create-item.png)

Insert a number into the UserID field, and a location string into the Location field. Click on the plus button to the left of Location, and choose Append, and select String. A new field will appear below Location. Call this field Username, and set the value to a username of your choosing. One major benefit of a NoSQL database is the ability to define fields on demand without having to manage updating large schema. This allows a developer to make an application decision at design time, and again at run time if necessary in order to store the necessary data for a new feature without having to retrofit existing databases, or having to update queries in the application. Click on the Save button, and you should see your newly input record in the Items list. 

{width=85%, float=center}
![DynamoDB Items Listing](images/chapter2.4-aws/dynamodb/dynamodb-item-listing.png)

Congratulations, you've now setup your first NoSQL database table for use in the application. 

*** EVALUATE AFTER WRITING SECTION ON ECS AND DOCKER ***
### 2.4.9 Additional Services
AWS is packed with services for all of your application needs, and they add more every year. It's beyond the scope of this book to add every valuable service that AWS has to offer, but I wanted to make sure I touched on a few that may not be used with our application, but would invaluable in a production environment. 

When designing the material for this book, I was tempted to use Elastic Beanstalk (EB) for Docker Container deployment, however, for simplicity sake, I decided to stick with a vanilla non-managed EC2 instance in order to show how easy it is to setup a new server manually. In a production environment, I would almost always choose to use EB in order to scale the application on demand. EB is essentially an automated wrapper to EC2 provided additional functionality to manage application deployment, and scaling logic. For example, if you have an application that normally requires 5 servers to handle, however you see spiked traffic occassionaly requiring 10 servers to handle, with EB it's not necessary to run 10 servers all the time in anticipation of the traffic, when for most of the time, you'd have to pay for 5 servers sitting idly by. EB allows you to specify criteria for when to scale, and what limitations to set so you don't accidentally add 100 servers when you meant to cap out at 10. If you want more information on EB, you can find it at <https://docs.aws.amazon.com/elastic-beanstalk/index.html>.

Another service I decided to skip is the Relation Database Service (RDS). This is service that provides access to a traditional SQL database, such as MySQL, Oracle, PostgreSQL, Microsoft SQL Server, AuroraDB, and MariaDB. I chose to design the application to use a 100% NoSQL datastore. I could have easily swapped out the datastore, but there has been so much information made available about RDS schema design and how that fits into the data tier of a full stack, I wanted to show how, with some foresight, and some thought into key design, a developer could build an entire application without relying on a relational daabase. If you wnat more information about RDS, you can find it at  <https://docs.aws.amazon.com/rds/index.html>.

And finally, I decided to not include Lambda. Lambda is an On-Demand Serverless Computing Service. Unlike EC2 paired with EB, Lambda provides a platform for deploying individual functions rather than entire applications. This is even more efficient than EB scalability as mentioned above, since function demand drive scalability rather than server or application level criteria. The reason I avoided this was not because it wouldn't be a good architecture, but because the challenges of designing a fully serverless application are still up for debate, and being tested in the real world every day. There is a non-linearity of design involved with serverless applications, and need to manage, or at least think about application state at a level that may be a more advanced topic, and perhaps the idea for another book in the future. If you want more information about Lambda, you can find it at <https://docs.aws.amazon.com/lambda/index.html>.   

